{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:47.404208Z",
     "iopub.status.busy": "2020-12-01T23:30:47.403887Z",
     "iopub.status.idle": "2020-12-01T23:30:49.737717Z",
     "shell.execute_reply": "2020-12-01T23:30:49.736939Z",
     "shell.execute_reply.started": "2020-12-01T23:30:47.404127Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "#to speed up pandas operands\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.layers import Dense, Dropout, Bidirectional\n",
    "from keras.layers import GlobalAveragePooling1D, Flatten, Conv1D\n",
    "from keras.layers import MaxPooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "\n",
    "import keras.utils as ku \n",
    "\n",
    "\n",
    "from project5_utils import my_print\n",
    "from project5_utils import my_wait\n",
    "\n",
    "from MyMovieReview import MovieReview\n",
    "from MyMovieReview import MovieReviewGenerator\n",
    "\n",
    "from mymovie import Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:49.739051Z",
     "iopub.status.busy": "2020-12-01T23:30:49.738888Z",
     "iopub.status.idle": "2020-12-01T23:30:49.747317Z",
     "shell.execute_reply": "2020-12-01T23:30:49.746590Z",
     "shell.execute_reply.started": "2020-12-01T23:30:49.739032Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:49.749093Z",
     "iopub.status.busy": "2020-12-01T23:30:49.748903Z",
     "iopub.status.idle": "2020-12-01T23:30:49.769275Z",
     "shell.execute_reply": "2020-12-01T23:30:49.768424Z",
     "shell.execute_reply.started": "2020-12-01T23:30:49.749071Z"
    }
   },
   "outputs": [],
   "source": [
    "LOG_FILE = open(\"../Data/notebook.log\",\"w\")\n",
    "DEBUG = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:49.770919Z",
     "iopub.status.busy": "2020-12-01T23:30:49.770702Z",
     "iopub.status.idle": "2020-12-01T23:30:49.775040Z",
     "shell.execute_reply": "2020-12-01T23:30:49.774426Z",
     "shell.execute_reply.started": "2020-12-01T23:30:49.770891Z"
    }
   },
   "outputs": [],
   "source": [
    "#Setting these for now will adjust them\n",
    "VOCAB_SIZE = 19000\n",
    "EMBEDDING_DIM = 64\n",
    "MAX_LENGTH = 1000\n",
    "TRUC_TYPE = \"post\"\n",
    "PAD_TYPE = \"post\"\n",
    "#Out of Value Token -- or token to indicate a \n",
    "OOV_TOK = \"<OOV>\"\n",
    "\n",
    "TRAIN_SET_PERCENT =  90.0\n",
    "EPOCHS = 500\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "BATCH_SIZE = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T18:12:30.876888Z",
     "iopub.status.busy": "2020-11-30T18:12:30.876581Z",
     "iopub.status.idle": "2020-11-30T18:12:30.880535Z",
     "shell.execute_reply": "2020-11-30T18:12:30.879833Z",
     "shell.execute_reply.started": "2020-11-30T18:12:30.876808Z"
    }
   },
   "source": [
    "# Data Processing\n",
    "Get reviews from PKL files into format that can be tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:49.776017Z",
     "iopub.status.busy": "2020-12-01T23:30:49.775865Z",
     "iopub.status.idle": "2020-12-01T23:30:49.968460Z",
     "shell.execute_reply": "2020-12-01T23:30:49.967690Z",
     "shell.execute_reply.started": "2020-12-01T23:30:49.775998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movie reviews read: 11997\n"
     ]
    }
   ],
   "source": [
    "MOVIE_REVIEWS_PKL_FILE = open(\"../Data/Reviews_back4.pkl\",\"rb\")\n",
    "\n",
    "list_of_movie_reviews = []\n",
    "\n",
    "while 1:\n",
    "    try:\n",
    "        temp_review = pickle.load(MOVIE_REVIEWS_PKL_FILE)\n",
    "        list_of_movie_reviews.append(temp_review)\n",
    "        my_print(\"added this review to the list ==>\\n{}\\n\".format(temp_review), DEBUG, LOG_FILE)\n",
    "    except EOFError:\n",
    "        my_print(\"DONE reading movie reviews\", DEBUG, LOG_FILE)\n",
    "        break\n",
    "        \n",
    "MOVIE_REVIEWS_PKL_FILE.close()\n",
    "print(\"Number of movie reviews read: {}\".format(len(list_of_movie_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:49.969489Z",
     "iopub.status.busy": "2020-12-01T23:30:49.969327Z",
     "iopub.status.idle": "2020-12-01T23:30:49.972454Z",
     "shell.execute_reply": "2020-12-01T23:30:49.971852Z",
     "shell.execute_reply.started": "2020-12-01T23:30:49.969469Z"
    }
   },
   "outputs": [],
   "source": [
    "col_names = list(list_of_movie_reviews[0].__dict__.keys())\n",
    "#col_values = list(list_of_movie_reviews[0].__dict__.values()) --> TODO: REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:49.973322Z",
     "iopub.status.busy": "2020-12-01T23:30:49.973174Z",
     "iopub.status.idle": "2020-12-01T23:30:50.029882Z",
     "shell.execute_reply": "2020-12-01T23:30:50.029214Z",
     "shell.execute_reply.started": "2020-12-01T23:30:49.973304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_star_rating</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>directlink_url</th>\n",
       "      <th>reviewlink_url</th>\n",
       "      <th>title</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the end of this movie there's a shot of the...</td>\n",
       "      <td>2</td>\n",
       "      <td>veardleyw</td>\n",
       "      <td>http://www.imdb.com/title/tt4669788/</td>\n",
       "      <td>http://www.imdb.com/review/rw4675276/</td>\n",
       "      <td>On the Basis of Sex</td>\n",
       "      <td>Feeble, telemovie-style travesty of a potentia...</td>\n",
       "      <td>20 February 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The film starts in 1956 with RBG (Felicity Jon...</td>\n",
       "      <td>8</td>\n",
       "      <td>nogodnomasters</td>\n",
       "      <td>http://www.imdb.com/title/tt4669788/</td>\n",
       "      <td>http://www.imdb.com/review/rw4783407/</td>\n",
       "      <td>On the Basis of Sex</td>\n",
       "      <td>It's not a movement if everyone is just sitting.</td>\n",
       "      <td>15 April 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This well meaning movie shows the discriminati...</td>\n",
       "      <td>5</td>\n",
       "      <td>phd_travel</td>\n",
       "      <td>http://www.imdb.com/title/tt4669788/</td>\n",
       "      <td>http://www.imdb.com/review/rw4590175/</td>\n",
       "      <td>On the Basis of Sex</td>\n",
       "      <td>Well intentioned but miscast</td>\n",
       "      <td>18 January 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why would you make a movie about the second fe...</td>\n",
       "      <td>1</td>\n",
       "      <td>Viking131313</td>\n",
       "      <td>http://www.imdb.com/title/tt4669788/</td>\n",
       "      <td>http://www.imdb.com/review/rw4621289/</td>\n",
       "      <td>On the Basis of Sex</td>\n",
       "      <td>Makes no sense.</td>\n",
       "      <td>2 February 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie was very well done. Good acting and...</td>\n",
       "      <td>10</td>\n",
       "      <td>MikeChm</td>\n",
       "      <td>http://www.imdb.com/title/tt4669788/</td>\n",
       "      <td>http://www.imdb.com/review/rw4583636/</td>\n",
       "      <td>On the Basis of Sex</td>\n",
       "      <td>Well worth going to see!</td>\n",
       "      <td>15 January 2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  review_star_rating  \\\n",
       "0  At the end of this movie there's a shot of the...                   2   \n",
       "1  The film starts in 1956 with RBG (Felicity Jon...                   8   \n",
       "2  This well meaning movie shows the discriminati...                   5   \n",
       "3  Why would you make a movie about the second fe...                   1   \n",
       "4  This movie was very well done. Good acting and...                  10   \n",
       "\n",
       "    reviewer_name                        directlink_url  \\\n",
       "0       veardleyw  http://www.imdb.com/title/tt4669788/   \n",
       "1  nogodnomasters  http://www.imdb.com/title/tt4669788/   \n",
       "2      phd_travel  http://www.imdb.com/title/tt4669788/   \n",
       "3    Viking131313  http://www.imdb.com/title/tt4669788/   \n",
       "4         MikeChm  http://www.imdb.com/title/tt4669788/   \n",
       "\n",
       "                          reviewlink_url                title  \\\n",
       "0  http://www.imdb.com/review/rw4675276/  On the Basis of Sex   \n",
       "1  http://www.imdb.com/review/rw4783407/  On the Basis of Sex   \n",
       "2  http://www.imdb.com/review/rw4590175/  On the Basis of Sex   \n",
       "3  http://www.imdb.com/review/rw4621289/  On the Basis of Sex   \n",
       "4  http://www.imdb.com/review/rw4583636/  On the Basis of Sex   \n",
       "\n",
       "                                        review_title       review_date  \n",
       "0  Feeble, telemovie-style travesty of a potentia...  20 February 2019  \n",
       "1   It's not a movement if everyone is just sitting.     15 April 2019  \n",
       "2                       Well intentioned but miscast   18 January 2019  \n",
       "3                                    Makes no sense.   2 February 2019  \n",
       "4                           Well worth going to see!   15 January 2019  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_movie_review_lists = []\n",
    "\n",
    "for review_col_name in col_names:\n",
    "    list_of_movie_review_data_vals = []\n",
    "    \n",
    "    for review_obj in list_of_movie_reviews:\n",
    "        list_of_movie_review_data_vals.append ( review_obj.__dict__[review_col_name] )\n",
    "        \n",
    "    list_of_movie_review_lists.append(list_of_movie_review_data_vals)\n",
    "    \n",
    "reviews_dict = dict(zip(col_names,list_of_movie_review_lists))\n",
    "reviews_df = pd.DataFrame(reviews_dict)\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUICK AND DIRTY CLEANING**\n",
    "\n",
    "TODO: Go back and Improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:50.030898Z",
     "iopub.status.busy": "2020-12-01T23:30:50.030744Z",
     "iopub.status.idle": "2020-12-01T23:30:50.079023Z",
     "shell.execute_reply": "2020-12-01T23:30:50.078273Z",
     "shell.execute_reply.started": "2020-12-01T23:30:50.030878Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get rid of instances of people spamming the same review\n",
    "reviews_df.drop_duplicates(subset=[\"review_text\"],keep=\"first\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:50.081179Z",
     "iopub.status.busy": "2020-12-01T23:30:50.081023Z",
     "iopub.status.idle": "2020-12-01T23:30:50.091717Z",
     "shell.execute_reply": "2020-12-01T23:30:50.090892Z",
     "shell.execute_reply.started": "2020-12-01T23:30:50.081160Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get rid of reviews where \"review_star_rating\" is missing or \"review_text\" is missing\n",
    "mask = (reviews_df[\"review_text\"] == \"EMPTY\") | (reviews_df[\"review_star_rating\"] == -1)\n",
    "mask_keep = ~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:50.092952Z",
     "iopub.status.busy": "2020-12-01T23:30:50.092803Z",
     "iopub.status.idle": "2020-12-01T23:30:50.099315Z",
     "shell.execute_reply": "2020-12-01T23:30:50.098657Z",
     "shell.execute_reply.started": "2020-12-01T23:30:50.092934Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews_df = reviews_df[mask_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:50.100583Z",
     "iopub.status.busy": "2020-12-01T23:30:50.100407Z",
     "iopub.status.idle": "2020-12-01T23:30:50.115955Z",
     "shell.execute_reply": "2020-12-01T23:30:50.115126Z",
     "shell.execute_reply.started": "2020-12-01T23:30:50.100561Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:50.117018Z",
     "iopub.status.busy": "2020-12-01T23:30:50.116852Z",
     "iopub.status.idle": "2020-12-01T23:30:50.121438Z",
     "shell.execute_reply": "2020-12-01T23:30:50.120755Z",
     "shell.execute_reply.started": "2020-12-01T23:30:50.116995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11962, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:50.122426Z",
     "iopub.status.busy": "2020-12-01T23:30:50.122268Z",
     "iopub.status.idle": "2020-12-01T23:30:50.131650Z",
     "shell.execute_reply": "2020-12-01T23:30:50.130691Z",
     "shell.execute_reply.started": "2020-12-01T23:30:50.122407Z"
    }
   },
   "outputs": [],
   "source": [
    "text_of_reviews = reviews_df[[\"review_text\",\"review_star_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:50.132945Z",
     "iopub.status.busy": "2020-12-01T23:30:50.132737Z",
     "iopub.status.idle": "2020-12-01T23:30:50.143462Z",
     "shell.execute_reply": "2020-12-01T23:30:50.142700Z",
     "shell.execute_reply.started": "2020-12-01T23:30:50.132918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the end of this movie there's a shot of the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The film starts in 1956 with RBG (Felicity Jon...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This well meaning movie shows the discriminati...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why would you make a movie about the second fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie was very well done. Good acting and...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  review_star_rating\n",
       "0  At the end of this movie there's a shot of the...                   2\n",
       "1  The film starts in 1956 with RBG (Felicity Jon...                   8\n",
       "2  This well meaning movie shows the discriminati...                   5\n",
       "3  Why would you make a movie about the second fe...                   1\n",
       "4  This movie was very well done. Good acting and...                  10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_of_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:50.144505Z",
     "iopub.status.busy": "2020-12-01T23:30:50.144349Z",
     "iopub.status.idle": "2020-12-01T23:30:50.151691Z",
     "shell.execute_reply": "2020-12-01T23:30:50.150961Z",
     "shell.execute_reply.started": "2020-12-01T23:30:50.144486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11962"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_of_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Lemmatization\n",
    "To reduce dimentionality of the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:50.153058Z",
     "iopub.status.busy": "2020-12-01T23:30:50.152830Z",
     "iopub.status.idle": "2020-12-01T23:30:50.159435Z",
     "shell.execute_reply": "2020-12-01T23:30:50.158798Z",
     "shell.execute_reply.started": "2020-12-01T23:30:50.153030Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_html_punct(row):\n",
    "    \"\"\"This function removes HTML and punctuation and anything that is or contains a number from the Text\"\"\"\n",
    "    \n",
    "    NLTK_WORDS = set(nltk.corpus.words.words())\n",
    "    \n",
    "    text_to_process = row[\"review_text\"]\n",
    "    text_to_process = text_to_process.lower()\n",
    "    text_to_process = re.sub(\"<.*?>\",\"\",text_to_process)\n",
    "    text_to_process = re.sub(\"[\\.|\\!|\\?|\\,|\\;|\\:|\\&|\\(|\\)|\\-|\\%|_|\\#|\\$|\\*|\\+|\\/|\\=|\\[|\\]|\\^|\\`|\\{|\\}|\\~]\",\"\",text_to_process)\n",
    "    text_to_process = re.sub(\"\\w*\\d+\\w*\",\"\",text_to_process)\n",
    "    text_to_process_list = text_to_process.split()\n",
    "    text_to_process_list = [review_word for review_word in text_to_process_list if review_word in NLTK_WORDS]\n",
    "    text_to_process = \" \".join(text_to_process_list)\n",
    "    return text_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:30:50.160355Z",
     "iopub.status.busy": "2020-12-01T23:30:50.160204Z",
     "iopub.status.idle": "2020-12-01T23:33:06.849167Z",
     "shell.execute_reply": "2020-12-01T23:33:06.848274Z",
     "shell.execute_reply.started": "2020-12-01T23:30:50.160335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 64 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "time spent doing operation is 0:02:16.679567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-a55e99bc47d3>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_of_reviews[\"review_text\"] = text_of_reviews.parallel_apply(remove_html_punct, axis=1)\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize()\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "text_of_reviews[\"review_text\"] = text_of_reviews.parallel_apply(remove_html_punct, axis=1)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"time spent doing operation is {}\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:06.851169Z",
     "iopub.status.busy": "2020-12-01T23:33:06.850834Z",
     "iopub.status.idle": "2020-12-01T23:33:06.881148Z",
     "shell.execute_reply": "2020-12-01T23:33:06.880300Z",
     "shell.execute_reply.started": "2020-12-01T23:33:06.851135Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-6d6c3b513ca5>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_of_reviews[\"review_text\"] = text_of_reviews[\"review_text\"].apply(lambda x : x.lower())\n"
     ]
    }
   ],
   "source": [
    "text_of_reviews[\"review_text\"] = text_of_reviews[\"review_text\"].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:06.882276Z",
     "iopub.status.busy": "2020-12-01T23:33:06.882113Z",
     "iopub.status.idle": "2020-12-01T23:33:06.890764Z",
     "shell.execute_reply": "2020-12-01T23:33:06.890065Z",
     "shell.execute_reply.started": "2020-12-01T23:33:06.882256Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_text(row):\n",
    "    temp_tokenized_txt = word_tokenize(row[\"review_text\"])\n",
    "    return temp_tokenized_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:06.891938Z",
     "iopub.status.busy": "2020-12-01T23:33:06.891734Z",
     "iopub.status.idle": "2020-12-01T23:33:08.491334Z",
     "shell.execute_reply": "2020-12-01T23:33:08.490451Z",
     "shell.execute_reply.started": "2020-12-01T23:33:06.891913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 64 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "time spent doing operation is 0:00:01.592295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-4e3f11e617c9>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_of_reviews[\"review_text\"] = text_of_reviews.parallel_apply(tokenize_text,axis=1)\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize()\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "text_of_reviews[\"review_text\"] = text_of_reviews.parallel_apply(tokenize_text,axis=1)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"time spent doing operation is {}\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:08.492473Z",
     "iopub.status.busy": "2020-12-01T23:33:08.492310Z",
     "iopub.status.idle": "2020-12-01T23:33:08.495365Z",
     "shell.execute_reply": "2020-12-01T23:33:08.494717Z",
     "shell.execute_reply.started": "2020-12-01T23:33:08.492453Z"
    }
   },
   "outputs": [],
   "source": [
    "wordNetLemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:08.496490Z",
     "iopub.status.busy": "2020-12-01T23:33:08.496341Z",
     "iopub.status.idle": "2020-12-01T23:33:08.504497Z",
     "shell.execute_reply": "2020-12-01T23:33:08.503901Z",
     "shell.execute_reply.started": "2020-12-01T23:33:08.496472Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_partofspeech(raw_pos):\n",
    "    \"\"\"translates from POS generated by pos_tag() to a POS encoding that WordNetLemmatizer.lemmatize() understands\"\"\"\n",
    "    #print(\"word = \",word)\n",
    "    #[(output_word, output_pos)] = pos_tag(word_tokenize(word))\n",
    "    \n",
    "    #decode output_pos to the pos required by the lemmatizer\n",
    "    \n",
    "    if \"JJ\" in raw_pos:\n",
    "        pos = \"a\"\n",
    "    elif \"RB\" in raw_pos:\n",
    "        pos = \"r\"\n",
    "    elif \"VB\" in raw_pos:\n",
    "        pos = \"v\"\n",
    "    else:\n",
    "        pos = \"n\"\n",
    "    \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:08.505341Z",
     "iopub.status.busy": "2020-12-01T23:33:08.505202Z",
     "iopub.status.idle": "2020-12-01T23:33:08.512687Z",
     "shell.execute_reply": "2020-12-01T23:33:08.511760Z",
     "shell.execute_reply.started": "2020-12-01T23:33:08.505323Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_it(row):\n",
    "    \n",
    "    pos_result = pos_tag(row[\"review_text\"])\n",
    "    \n",
    "    temp_mydoc_lemmatized = []\n",
    "    \n",
    "    for myword,myPOS in pos_result:\n",
    "        temp_word_lemmatized = wordNetLemmatizer.lemmatize(myword,calc_partofspeech(myPOS))\n",
    "        temp_mydoc_lemmatized.append(temp_word_lemmatized)\n",
    "    \n",
    "    return temp_mydoc_lemmatized\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:43.186022Z",
     "iopub.status.busy": "2020-12-01T23:33:43.185761Z",
     "iopub.status.idle": "2020-12-01T23:33:57.342908Z",
     "shell.execute_reply": "2020-12-01T23:33:57.341873Z",
     "shell.execute_reply.started": "2020-12-01T23:33:43.185995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 64 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "time spent doing operation is 0:00:14.150488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-a3ec05ecf6e3>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_of_reviews[\"review_text\"] = text_of_reviews.parallel_apply(lemmatize_it,axis=1)\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize()\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "text_of_reviews[\"review_text\"] = text_of_reviews.parallel_apply(lemmatize_it,axis=1)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"time spent doing operation is {}\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:57.345190Z",
     "iopub.status.busy": "2020-12-01T23:33:57.344938Z",
     "iopub.status.idle": "2020-12-01T23:33:58.777572Z",
     "shell.execute_reply": "2020-12-01T23:33:58.776682Z",
     "shell.execute_reply.started": "2020-12-01T23:33:57.345161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 64 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-f2a5d8e08abf>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_of_reviews[\"review_text\"]  = text_of_reviews.parallel_apply(lambda x : \" \".join(x[\"review_text\"]), axis=1)\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize()\n",
    "start_time = datetime.datetime.now()\n",
    "text_of_reviews[\"review_text\"]  = text_of_reviews.parallel_apply(lambda x : \" \".join(x[\"review_text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:58.779121Z",
     "iopub.status.busy": "2020-12-01T23:33:58.778960Z",
     "iopub.status.idle": "2020-12-01T23:33:58.787198Z",
     "shell.execute_reply": "2020-12-01T23:33:58.786406Z",
     "shell.execute_reply.started": "2020-12-01T23:33:58.779101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>at the end of this movie a shot of the real ch...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the film in with felicity enter law school a o...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this well mean movie the discrimination face a...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why would you make a movie about the second fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this movie be very well do good acting and a g...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  review_star_rating\n",
       "0  at the end of this movie a shot of the real ch...                   2\n",
       "1  the film in with felicity enter law school a o...                   8\n",
       "2  this well mean movie the discrimination face a...                   5\n",
       "3  why would you make a movie about the second fe...                   1\n",
       "4  this movie be very well do good acting and a g...                  10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_of_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:58.788525Z",
     "iopub.status.busy": "2020-12-01T23:33:58.788371Z",
     "iopub.status.idle": "2020-12-01T23:33:58.798021Z",
     "shell.execute_reply": "2020-12-01T23:33:58.797249Z",
     "shell.execute_reply.started": "2020-12-01T23:33:58.788507Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-fd720e7df4eb>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_of_reviews[\"review_star_rating\"] = text_of_reviews[\"review_star_rating\"] - 1\n"
     ]
    }
   ],
   "source": [
    "#shifting range of ratings from 1,10 to 0,9\n",
    "text_of_reviews[\"review_star_rating\"] = text_of_reviews[\"review_star_rating\"] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:58.799335Z",
     "iopub.status.busy": "2020-12-01T23:33:58.799126Z",
     "iopub.status.idle": "2020-12-01T23:33:58.803979Z",
     "shell.execute_reply": "2020-12-01T23:33:58.803406Z",
     "shell.execute_reply.started": "2020-12-01T23:33:58.799308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11962, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_of_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:58.804896Z",
     "iopub.status.busy": "2020-12-01T23:33:58.804749Z",
     "iopub.status.idle": "2020-12-01T23:33:58.813288Z",
     "shell.execute_reply": "2020-12-01T23:33:58.812563Z",
     "shell.execute_reply.started": "2020-12-01T23:33:58.804878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10765\n",
      "(11962, 2)\n",
      "(10765,)\n",
      "(1197,)\n"
     ]
    }
   ],
   "source": [
    "#TODO Randomize sets or find tensorflow train test split or use sklearn train test split\n",
    "train_set_size = int ( ( text_of_reviews.shape[0] * TRAIN_SET_PERCENT ) / 100 )\n",
    "#test_set_size =  text_of_reviews.shape[0] - train_set_size\n",
    "\n",
    "review_indices = np.arange(text_of_reviews.shape[0]) \n",
    "\n",
    "np.random.shuffle( review_indices)\n",
    "\n",
    "\n",
    "text_of_reviews = text_of_reviews.iloc[review_indices]\n",
    "\n",
    "training_indices = review_indices[:train_set_size]\n",
    "testing_indices = review_indices[train_set_size:]\n",
    "\n",
    "\n",
    "print(train_set_size)\n",
    "print(text_of_reviews.shape)\n",
    "print(training_indices.shape)\n",
    "print(testing_indices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:58.814372Z",
     "iopub.status.busy": "2020-12-01T23:33:58.814211Z",
     "iopub.status.idle": "2020-12-01T23:33:58.822212Z",
     "shell.execute_reply": "2020-12-01T23:33:58.821568Z",
     "shell.execute_reply.started": "2020-12-01T23:33:58.814351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>with four range from i have see a lot of and a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8508</th>\n",
       "      <td>to all the who give this movie low i can only ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9111</th>\n",
       "      <td>the the battle of the five be the of all peter...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8892</th>\n",
       "      <td>the year be and a remote north eastern coastal...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10218</th>\n",
       "      <td>i guess you know the drill already with the lo...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             review_text  review_star_rating\n",
       "5099   with four range from i have see a lot of and a...                   8\n",
       "8508   to all the who give this movie low i can only ...                   6\n",
       "9111   the the battle of the five be the of all peter...                   4\n",
       "8892   the year be and a remote north eastern coastal...                   7\n",
       "10218  i guess you know the drill already with the lo...                   6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_of_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:58.823132Z",
     "iopub.status.busy": "2020-12-01T23:33:58.822983Z",
     "iopub.status.idle": "2020-12-01T23:33:58.830868Z",
     "shell.execute_reply": "2020-12-01T23:33:58.830167Z",
     "shell.execute_reply.started": "2020-12-01T23:33:58.823113Z"
    }
   },
   "outputs": [],
   "source": [
    "#spent hours trying to re-implement train_test_split() :*-(\n",
    "\n",
    "#train_new_reviews = text_of_reviews.loc[text_of_reviews.index.intersection(training_indices),\"review_text\"]\n",
    "#train_new_reviews_rating = text_of_reviews.loc[text_of_reviews.index.intersection(training_indices),\"review_star_rating\"]\n",
    "\n",
    "#train_text_reviews = text_of_reviews.sample(n=train_set_size, random_state=RANDOM_STATE, )\n",
    "\n",
    "#test_new_reviews = text_of_reviews.loc[text_of_reviews.index.intersection(testing_indices),\"review_text\"]\n",
    "#test_new_reviews_rating = text_of_reviews.loc[text_of_reviews.index.intersection(testing_indices),\"review_star_rating\"]\n",
    "\n",
    "X = text_of_reviews[\"review_text\"]\n",
    "y = text_of_reviews[\"review_star_rating\"]\n",
    "\n",
    "Test_Size = 1.0 - (TRAIN_SET_PERCENT/100.0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=Test_Size, random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:58.833171Z",
     "iopub.status.busy": "2020-12-01T23:33:58.833014Z",
     "iopub.status.idle": "2020-12-01T23:33:58.837718Z",
     "shell.execute_reply": "2020-12-01T23:33:58.837062Z",
     "shell.execute_reply.started": "2020-12-01T23:33:58.833152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10765,)\n",
      "(10765,)\n",
      "(1197,)\n",
      "(1197,)\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "#Lemmatization\n",
    "#stop word removal\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:33:58.839057Z",
     "iopub.status.busy": "2020-12-01T23:33:58.838910Z",
     "iopub.status.idle": "2020-12-01T23:34:00.156244Z",
     "shell.execute_reply": "2020-12-01T23:34:00.155462Z",
     "shell.execute_reply.started": "2020-12-01T23:33:58.839039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index[<OOV>] = 1\n",
      "word_index[the] = 2\n",
      "word_index[be] = 3\n",
      "word_index[a] = 4\n",
      "word_index[and] = 5\n",
      "word_index[to] = 6\n",
      "word_index[of] = 7\n",
      "word_index[in] = 8\n",
      "word_index[it] = 9\n",
      "word_index[i] = 10\n",
      "\n",
      "total words is 19281\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token=OOV_TOK, num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "total_words = len(tokenizer.word_index)+1\n",
    "i = 0\n",
    "for key, value in tokenizer.word_index.items():\n",
    "    \n",
    "    if i == 10:\n",
    "        break\n",
    "        \n",
    "    print(\"word_index[{}] = {}\".format(key, value))\n",
    "    i += 1\n",
    "    \n",
    "print(\"\\ntotal words is {}\".format(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:00.157255Z",
     "iopub.status.busy": "2020-12-01T23:34:00.157098Z",
     "iopub.status.idle": "2020-12-01T23:34:01.306007Z",
     "shell.execute_reply": "2020-12-01T23:34:01.305000Z",
     "shell.execute_reply.started": "2020-12-01T23:34:00.157236Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_seq_pad = pad_sequences(X_train_seq, maxlen=MAX_LENGTH, padding=PAD_TYPE, truncating=TRUC_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:01.307461Z",
     "iopub.status.busy": "2020-12-01T23:34:01.307272Z",
     "iopub.status.idle": "2020-12-01T23:34:01.440197Z",
     "shell.execute_reply": "2020-12-01T23:34:01.439496Z",
     "shell.execute_reply.started": "2020-12-01T23:34:01.307431Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_seq_pad = pad_sequences(X_test_seq, maxlen=MAX_LENGTH, padding=PAD_TYPE, truncating=TRUC_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:01.441283Z",
     "iopub.status.busy": "2020-12-01T23:34:01.441121Z",
     "iopub.status.idle": "2020-12-01T23:34:01.445675Z",
     "shell.execute_reply": "2020-12-01T23:34:01.444980Z",
     "shell.execute_reply.started": "2020-12-01T23:34:01.441263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thing = [12, 17, 3, 266, 123, 2, 5319, 34, 34, 5, 34, 9, 4, 351, 37, 3, 1785, 131, 6, 260, 6, 419, 5, 1045, 90, 6, 11, 255, 24, 274, 61, 170, 20, 12, 3, 49, 462, 482, 11, 3, 1035, 4021, 34, 2, 25, 3, 70, 16, 8, 646, 5, 3, 58, 1669, 3, 11, 2, 3875, 113, 3, 37, 10, 63, 3, 2, 2546, 4239, 268, 1421, 16, 44, 80, 49, 41, 928, 93, 8, 646, 44, 3, 12, 113, 166, 99, 34, 49, 5, 20, 62, 33, 170, 24, 3, 73, 23, 10, 33, 23, 5, 107, 290, 80, 15, 54, 20, 35, 86, 510, 189, 124, 15, 22, 180, 23, 621, 14, 12, 17, 6, 659, 8, 4, 3715, 79, 381, 45, 9, 23, 144, 11, 213, 19, 3, 3077, 7, 3248, 16, 10, 73, 9, 101, 10, 73, 9, 6, 3573, 71, 272, 5, 243, 98, 90, 31, 24, 239, 3, 73, 23, 10, 42, 10, 80, 173, 2, 180, 23, 4, 36, 210, 7, 6, 1763, 20, 7, 4, 505, 45, 163, 15, 1748, 44, 3, 4, 113, 98, 3, 14, 22, 4580, 8, 2, 11935, 5, 96, 409, 99, 149, 1812, 113, 7, 44, 3, 4, 431, 7, 2, 1264, 14, 2, 11935, 90, 1776, 3, 81, 11, 24, 1863, 3370, 142, 6, 23, 12, 17, 4, 24, 1367, 23, 4, 878, 17, 1864, 236, 4997, 6478, 67, 3, 80, 266, 21, 22, 137, 118, 1953, 14, 5963, 16, 662, 443, 6, 23, 12, 27, 4, 24, 105, 55, 18, 144, 2, 87, 324, 7, 878, 1953, 10, 73, 24, 23, 4, 36, 210, 14, 9, 9, 105, 23, 16, 10, 80, 5, 11, 3, 8, 12, 287, 24, 3, 736, 22, 192, 1098, 16, 24, 4, 2, 357, 20, 115, 15, 86, 10, 172, 12, 17, 5, 10, 80, 73, 2, 3, 21, 161, 10, 73, 20, 63, 35, 12, 17, 1442, 59, 268, 1421, 255, 97, 478, 5, 97, 558, 16, 10, 73, 253, 15, 5, 9, 65, 3, 1281, 50, 26, 11936, 44, 149, 158, 10, 142, 6, 703, 3, 11, 40, 1632, 134, 2, 44, 3, 4, 23, 34, 98, 24, 3, 203, 48, 4, 6479, 11, 3, 2390, 29, 11, 213, 3, 120, 20, 142, 6, 611, 189, 15]\n",
      "thing = [396, 71, 4, 1390, 421, 26, 2788, 6, 39, 4, 1878, 7, 343, 5, 51, 43, 26, 746, 78, 2, 1390, 135, 6, 3, 4, 88, 103, 37, 4, 119, 32, 2483, 31, 794, 33, 149, 17, 635, 3, 53, 1136, 5, 33, 1122, 343, 29, 69, 29, 11, 138, 499, 8, 4, 801, 79, 770, 27, 7, 91, 10, 3, 25, 2254, 9]\n",
      "thing = [9, 61, 1685, 6, 170, 58, 2484, 2, 3129, 16, 23, 19, 1118, 2, 692, 34, 2, 75, 17, 8, 12, 8028, 362, 5, 172, 493, 2, 87, 7, 2, 7123, 1156, 173, 2, 12, 4346, 1229, 5722, 7537, 5723, 200, 31, 2, 1422, 7, 1610, 869, 1569, 5, 4998, 67, 29, 1896, 114, 57, 3, 14, 4, 1145, 3130, 5, 3715, 561, 14, 116, 1106, 15, 73, 4725, 4240, 12, 17, 3, 41, 3310, 6, 7, 72, 6, 168, 6, 9, 5964, 7, 7, 58, 832, 3, 384, 445, 6, 2, 520, 28, 2, 474, 7, 374, 100, 14, 198, 515, 116, 1220, 10, 115, 63, 93, 472, 5320, 2, 17, 4, 26, 3, 120, 202, 16, 759, 82, 17, 101, 667, 10, 62, 241, 58, 825, 5, 1111, 2, 17, 7, 2, 75, 63, 3, 847, 6, 648, 11, 146, 3574, 4123, 4, 698, 6, 572, 68, 3, 261, 2896, 5, 113, 99, 113, 4, 230, 928, 164, 5, 384, 4, 178, 93, 1090, 5, 44, 3, 127, 6, 12, 143, 2, 3021, 2423, 5, 4347, 21, 22, 171, 15, 4, 1285, 1230, 16, 24, 3, 89, 93, 125, 4, 184, 22, 164, 3, 704, 93, 1016, 5, 24, 261, 23, 19, 2391, 2, 223, 707, 6, 756, 104, 3, 6787, 112, 2, 75, 4, 2, 7, 4, 3716, 7, 3644, 11, 3, 3078, 8, 2, 75, 518, 2, 3, 215, 72, 152, 5, 320, 49, 5, 269, 2, 79, 662, 110, 102, 63, 1749, 3, 34, 2, 3941, 5, 1384, 5724, 374, 2, 339, 100, 9, 3, 435, 11, 2392, 5, 1724, 929, 114, 11, 24, 1813, 2, 17, 3, 3787, 458, 34, 5, 8689, 341, 34, 14265, 107, 5, 3, 6, 51, 123, 3079, 10, 204, 23, 5, 2, 192, 262, 6, 61, 361, 6, 237, 439, 103, 66]\n"
     ]
    }
   ],
   "source": [
    "for i, thing in enumerate(X_train_seq):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(\"thing = {}\".format(thing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:01.446579Z",
     "iopub.status.busy": "2020-12-01T23:34:01.446425Z",
     "iopub.status.idle": "2020-12-01T23:34:01.452895Z",
     "shell.execute_reply": "2020-12-01T23:34:01.452264Z",
     "shell.execute_reply.started": "2020-12-01T23:34:01.446560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10765, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:01.454096Z",
     "iopub.status.busy": "2020-12-01T23:34:01.453894Z",
     "iopub.status.idle": "2020-12-01T23:34:01.468840Z",
     "shell.execute_reply": "2020-12-01T23:34:01.468138Z",
     "shell.execute_reply.started": "2020-12-01T23:34:01.454070Z"
    }
   },
   "outputs": [],
   "source": [
    "#Convert to numpy arrays to work with Tensorflow\n",
    "X_train_seq_pad = np.array(X_train_seq_pad)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test_seq_pad = np.array(X_test_seq_pad)\n",
    "y_test = np.array(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:01.469836Z",
     "iopub.status.busy": "2020-12-01T23:34:01.469687Z",
     "iopub.status.idle": "2020-12-01T23:34:01.473950Z",
     "shell.execute_reply": "2020-12-01T23:34:01.473312Z",
     "shell.execute_reply.started": "2020-12-01T23:34:01.469816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10765, 1000), (10765,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq_pad.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:01.474827Z",
     "iopub.status.busy": "2020-12-01T23:34:01.474681Z",
     "iopub.status.idle": "2020-12-01T23:34:01.480330Z",
     "shell.execute_reply": "2020-12-01T23:34:01.479734Z",
     "shell.execute_reply.started": "2020-12-01T23:34:01.474809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1197, 1000), (1197,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_seq_pad.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:01.481197Z",
     "iopub.status.busy": "2020-12-01T23:34:01.481053Z",
     "iopub.status.idle": "2020-12-01T23:34:01.488065Z",
     "shell.execute_reply": "2020-12-01T23:34:01.487320Z",
     "shell.execute_reply.started": "2020-12-01T23:34:01.481179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6681,   25,    7, ...,    0,    0,    0],\n",
       "       [  35,    2,   13, ...,    0,    0,    0],\n",
       "       [  12,   13,    3, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_seq_pad[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:01.489338Z",
     "iopub.status.busy": "2020-12-01T23:34:01.489084Z",
     "iopub.status.idle": "2020-12-01T23:34:01.494691Z",
     "shell.execute_reply": "2020-12-01T23:34:01.494088Z",
     "shell.execute_reply.started": "2020-12-01T23:34:01.489312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 7])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T18:12:45.748845Z",
     "iopub.status.busy": "2020-11-30T18:12:45.748563Z",
     "iopub.status.idle": "2020-11-30T18:12:45.751628Z",
     "shell.execute_reply": "2020-11-30T18:12:45.750984Z",
     "shell.execute_reply.started": "2020-11-30T18:12:45.748814Z"
    }
   },
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:01.495763Z",
     "iopub.status.busy": "2020-12-01T23:34:01.495612Z",
     "iopub.status.idle": "2020-12-01T23:34:04.106210Z",
     "shell.execute_reply": "2020-12-01T23:34:04.105410Z",
     "shell.execute_reply.started": "2020-12-01T23:34:01.495745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1000, 64)          1216064   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 994, 32)           14368     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 198, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 192, 32)           7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 15)                495       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                160       \n",
      "=================================================================\n",
      "Total params: 1,238,287\n",
      "Trainable params: 1,238,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#baseline_model = Sequential([\n",
    "#    Embedding(VOCAB_SIZE + 1,EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "#    GlobalAveragePooling1D(),\n",
    "#    Dense(500, activation=\"relu\"),\n",
    "#    Dense(250, activation=\"relu\"),\n",
    "#    Dense(125, activation=\"relu\"),\n",
    "#    Dense(60, activation=\"relu\"),\n",
    "#    Dense(30, activation=\"relu\"),\n",
    "#    Dense(15, activation=\"relu\"),\n",
    "#    Dense(10, activation=\"softmax\")\n",
    "#])\n",
    "\n",
    "baseline_model = Sequential()\n",
    "baseline_model.add(Embedding(VOCAB_SIZE+1, EMBEDDING_DIM, input_length=MAX_LENGTH))\n",
    "baseline_model.add(Conv1D(32,7,activation=\"relu\"))\n",
    "baseline_model.add(MaxPooling1D(5))\n",
    "baseline_model.add(Conv1D(32,7,activation=\"relu\"))\n",
    "baseline_model.add(GlobalMaxPooling1D())\n",
    "baseline_model.add(Dense(15, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)))\n",
    "baseline_model.add(Dropout(0.2))\n",
    "baseline_model.add(Dense(10, activation=\"softmax\", kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "baseline_model.compile(optimizer=\"adadelta\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:04.107326Z",
     "iopub.status.busy": "2020-12-01T23:34:04.107164Z",
     "iopub.status.idle": "2020-12-01T23:34:04.110864Z",
     "shell.execute_reply": "2020-12-01T23:34:04.110229Z",
     "shell.execute_reply.started": "2020-12-01T23:34:04.107307Z"
    }
   },
   "outputs": [],
   "source": [
    "call_back_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = \"../Data/baseline_model.h5\",\n",
    "        monitor = \"val_loss\",\n",
    "        save_best_only = True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor = \"val_loss\",\n",
    "        factor = 0.1,\n",
    "        patience = 10\n",
    "    )\n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T23:34:04.111806Z",
     "iopub.status.busy": "2020-12-01T23:34:04.111652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "101/101 [==============================] - 2s 23ms/step - loss: 2.3320 - acc: 0.1382 - val_loss: 2.3325 - val_acc: 0.1504 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "101/101 [==============================] - 2s 20ms/step - loss: 2.3322 - acc: 0.1359 - val_loss: 2.3322 - val_acc: 0.1513 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3319 - acc: 0.1422 - val_loss: 2.3319 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3316 - acc: 0.1395 - val_loss: 2.3317 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3317 - acc: 0.1369 - val_loss: 2.3314 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3309 - acc: 0.1418 - val_loss: 2.3311 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "101/101 [==============================] - 2s 20ms/step - loss: 2.3310 - acc: 0.1360 - val_loss: 2.3308 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3307 - acc: 0.1406 - val_loss: 2.3305 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3304 - acc: 0.1413 - val_loss: 2.3302 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3301 - acc: 0.1417 - val_loss: 2.3299 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3300 - acc: 0.1369 - val_loss: 2.3296 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3295 - acc: 0.1452 - val_loss: 2.3293 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "101/101 [==============================] - 2s 20ms/step - loss: 2.3295 - acc: 0.1379 - val_loss: 2.3290 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3293 - acc: 0.1447 - val_loss: 2.3287 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3291 - acc: 0.1371 - val_loss: 2.3284 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3287 - acc: 0.1433 - val_loss: 2.3281 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "101/101 [==============================] - 2s 20ms/step - loss: 2.3284 - acc: 0.1431 - val_loss: 2.3278 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3284 - acc: 0.1385 - val_loss: 2.3275 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3281 - acc: 0.1382 - val_loss: 2.3272 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3274 - acc: 0.1411 - val_loss: 2.3269 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3275 - acc: 0.1419 - val_loss: 2.3266 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3272 - acc: 0.1443 - val_loss: 2.3264 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3271 - acc: 0.1464 - val_loss: 2.3261 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3267 - acc: 0.1444 - val_loss: 2.3258 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3265 - acc: 0.1486 - val_loss: 2.3255 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3265 - acc: 0.1420 - val_loss: 2.3252 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3262 - acc: 0.1479 - val_loss: 2.3249 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3259 - acc: 0.1454 - val_loss: 2.3246 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3255 - acc: 0.1452 - val_loss: 2.3244 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3255 - acc: 0.1491 - val_loss: 2.3241 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3253 - acc: 0.1494 - val_loss: 2.3238 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3249 - acc: 0.1464 - val_loss: 2.3235 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3252 - acc: 0.1482 - val_loss: 2.3232 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3243 - acc: 0.1450 - val_loss: 2.3229 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3243 - acc: 0.1501 - val_loss: 2.3227 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3243 - acc: 0.1518 - val_loss: 2.3224 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3235 - acc: 0.1516 - val_loss: 2.3221 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3238 - acc: 0.1500 - val_loss: 2.3218 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3234 - acc: 0.1501 - val_loss: 2.3216 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3231 - acc: 0.1519 - val_loss: 2.3213 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3227 - acc: 0.1485 - val_loss: 2.3210 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3227 - acc: 0.1503 - val_loss: 2.3207 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3226 - acc: 0.1514 - val_loss: 2.3205 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3226 - acc: 0.1488 - val_loss: 2.3202 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3224 - acc: 0.1528 - val_loss: 2.3199 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3216 - acc: 0.1548 - val_loss: 2.3196 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3215 - acc: 0.1531 - val_loss: 2.3194 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3216 - acc: 0.1498 - val_loss: 2.3191 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3215 - acc: 0.1507 - val_loss: 2.3188 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3212 - acc: 0.1531 - val_loss: 2.3186 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3208 - acc: 0.1493 - val_loss: 2.3183 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3203 - acc: 0.1528 - val_loss: 2.3180 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3208 - acc: 0.1549 - val_loss: 2.3177 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3200 - acc: 0.1516 - val_loss: 2.3175 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3201 - acc: 0.1513 - val_loss: 2.3172 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3201 - acc: 0.1485 - val_loss: 2.3169 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3196 - acc: 0.1539 - val_loss: 2.3167 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3193 - acc: 0.1518 - val_loss: 2.3164 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3192 - acc: 0.1540 - val_loss: 2.3162 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3194 - acc: 0.1506 - val_loss: 2.3159 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3193 - acc: 0.1509 - val_loss: 2.3157 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3191 - acc: 0.1509 - val_loss: 2.3154 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3184 - acc: 0.1515 - val_loss: 2.3151 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3187 - acc: 0.1561 - val_loss: 2.3149 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3183 - acc: 0.1540 - val_loss: 2.3146 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3176 - acc: 0.1513 - val_loss: 2.3144 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3182 - acc: 0.1526 - val_loss: 2.3141 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3179 - acc: 0.1523 - val_loss: 2.3139 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3182 - acc: 0.1543 - val_loss: 2.3136 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3170 - acc: 0.1537 - val_loss: 2.3134 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3174 - acc: 0.1525 - val_loss: 2.3131 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3171 - acc: 0.1533 - val_loss: 2.3129 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3162 - acc: 0.1564 - val_loss: 2.3126 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3163 - acc: 0.1539 - val_loss: 2.3123 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3160 - acc: 0.1515 - val_loss: 2.3121 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3165 - acc: 0.1507 - val_loss: 2.3118 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3157 - acc: 0.1521 - val_loss: 2.3116 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3160 - acc: 0.1559 - val_loss: 2.3113 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3158 - acc: 0.1509 - val_loss: 2.3111 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3156 - acc: 0.1505 - val_loss: 2.3108 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3147 - acc: 0.1571 - val_loss: 2.3106 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3154 - acc: 0.1518 - val_loss: 2.3103 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3146 - acc: 0.1525 - val_loss: 2.3101 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3146 - acc: 0.1564 - val_loss: 2.3098 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3139 - acc: 0.1540 - val_loss: 2.3096 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3141 - acc: 0.1501 - val_loss: 2.3093 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3143 - acc: 0.1533 - val_loss: 2.3091 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3130 - acc: 0.1552 - val_loss: 2.3088 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3138 - acc: 0.1527 - val_loss: 2.3086 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3137 - acc: 0.1498 - val_loss: 2.3083 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3136 - acc: 0.1530 - val_loss: 2.3081 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3124 - acc: 0.1519 - val_loss: 2.3078 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3127 - acc: 0.1544 - val_loss: 2.3076 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3125 - acc: 0.1527 - val_loss: 2.3073 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3124 - acc: 0.1513 - val_loss: 2.3071 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3127 - acc: 0.1503 - val_loss: 2.3069 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3123 - acc: 0.1512 - val_loss: 2.3066 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3118 - acc: 0.1552 - val_loss: 2.3064 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3120 - acc: 0.1547 - val_loss: 2.3061 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3119 - acc: 0.1550 - val_loss: 2.3059 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3112 - acc: 0.1518 - val_loss: 2.3056 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3105 - acc: 0.1550 - val_loss: 2.3054 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3111 - acc: 0.1538 - val_loss: 2.3051 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3108 - acc: 0.1518 - val_loss: 2.3049 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3107 - acc: 0.1527 - val_loss: 2.3046 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3108 - acc: 0.1544 - val_loss: 2.3044 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3107 - acc: 0.1561 - val_loss: 2.3041 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3107 - acc: 0.1526 - val_loss: 2.3039 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3097 - acc: 0.1533 - val_loss: 2.3037 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 110/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3094 - acc: 0.1531 - val_loss: 2.3034 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 111/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3098 - acc: 0.1515 - val_loss: 2.3032 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 112/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3105 - acc: 0.1523 - val_loss: 2.3029 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 113/500\n",
      "101/101 [==============================] - 2s 20ms/step - loss: 2.3091 - acc: 0.1521 - val_loss: 2.3027 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 114/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3089 - acc: 0.1520 - val_loss: 2.3025 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 115/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3089 - acc: 0.1526 - val_loss: 2.3022 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 116/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3088 - acc: 0.1508 - val_loss: 2.3020 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 117/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3086 - acc: 0.1523 - val_loss: 2.3017 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 118/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3082 - acc: 0.1527 - val_loss: 2.3015 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 119/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3076 - acc: 0.1542 - val_loss: 2.3012 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 120/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3086 - acc: 0.1500 - val_loss: 2.3010 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 121/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3083 - acc: 0.1511 - val_loss: 2.3008 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 122/500\n",
      "101/101 [==============================] - 2s 20ms/step - loss: 2.3068 - acc: 0.1551 - val_loss: 2.3005 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 123/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3076 - acc: 0.1527 - val_loss: 2.3003 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 124/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3069 - acc: 0.1519 - val_loss: 2.3000 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 125/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3071 - acc: 0.1575 - val_loss: 2.2998 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 126/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3071 - acc: 0.1525 - val_loss: 2.2995 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 127/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3070 - acc: 0.1505 - val_loss: 2.2993 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 128/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3067 - acc: 0.1531 - val_loss: 2.2991 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 129/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3060 - acc: 0.1555 - val_loss: 2.2988 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 130/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3063 - acc: 0.1566 - val_loss: 2.2986 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 131/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3059 - acc: 0.1523 - val_loss: 2.2983 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 132/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3053 - acc: 0.1549 - val_loss: 2.2981 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 133/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3065 - acc: 0.1485 - val_loss: 2.2979 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 134/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3068 - acc: 0.1547 - val_loss: 2.2977 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 135/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3059 - acc: 0.1578 - val_loss: 2.2974 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 136/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3046 - acc: 0.1545 - val_loss: 2.2972 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 137/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3059 - acc: 0.1515 - val_loss: 2.2970 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 138/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3057 - acc: 0.1544 - val_loss: 2.2967 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 139/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3052 - acc: 0.1535 - val_loss: 2.2965 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 140/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3053 - acc: 0.1499 - val_loss: 2.2963 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 141/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3043 - acc: 0.1539 - val_loss: 2.2961 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 142/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3033 - acc: 0.1547 - val_loss: 2.2958 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 143/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3040 - acc: 0.1540 - val_loss: 2.2956 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 144/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3047 - acc: 0.1524 - val_loss: 2.2954 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 145/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3045 - acc: 0.1525 - val_loss: 2.2951 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 146/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3039 - acc: 0.1501 - val_loss: 2.2949 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 147/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3029 - acc: 0.1507 - val_loss: 2.2947 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 148/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3040 - acc: 0.1509 - val_loss: 2.2945 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 149/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3028 - acc: 0.1547 - val_loss: 2.2942 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 150/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3031 - acc: 0.1521 - val_loss: 2.2940 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 151/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3026 - acc: 0.1519 - val_loss: 2.2938 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 152/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3022 - acc: 0.1538 - val_loss: 2.2935 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 153/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3027 - acc: 0.1517 - val_loss: 2.2933 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 154/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3019 - acc: 0.1502 - val_loss: 2.2931 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 155/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3024 - acc: 0.1541 - val_loss: 2.2929 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 156/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3016 - acc: 0.1565 - val_loss: 2.2926 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 157/500\n",
      "101/101 [==============================] - 2s 20ms/step - loss: 2.3025 - acc: 0.1480 - val_loss: 2.2924 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 158/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3009 - acc: 0.1547 - val_loss: 2.2922 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 159/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3011 - acc: 0.1504 - val_loss: 2.2920 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 160/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3021 - acc: 0.1563 - val_loss: 2.2917 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 161/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3010 - acc: 0.1572 - val_loss: 2.2915 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 162/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3011 - acc: 0.1485 - val_loss: 2.2913 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 163/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3000 - acc: 0.1530 - val_loss: 2.2911 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 164/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3012 - acc: 0.1515 - val_loss: 2.2908 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 165/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3017 - acc: 0.1513 - val_loss: 2.2906 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 166/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2995 - acc: 0.1555 - val_loss: 2.2904 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 167/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3012 - acc: 0.1480 - val_loss: 2.2902 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 168/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3009 - acc: 0.1496 - val_loss: 2.2900 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 169/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3009 - acc: 0.1481 - val_loss: 2.2898 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 170/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.3004 - acc: 0.1483 - val_loss: 2.2895 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 171/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2996 - acc: 0.1531 - val_loss: 2.2893 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 172/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2998 - acc: 0.1531 - val_loss: 2.2891 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 173/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2976 - acc: 0.1532 - val_loss: 2.2889 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 174/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3001 - acc: 0.1507 - val_loss: 2.2887 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 175/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.3000 - acc: 0.1537 - val_loss: 2.2885 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 176/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2996 - acc: 0.1539 - val_loss: 2.2882 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 177/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2981 - acc: 0.1564 - val_loss: 2.2880 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 178/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2986 - acc: 0.1545 - val_loss: 2.2878 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 179/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2985 - acc: 0.1518 - val_loss: 2.2876 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 180/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2987 - acc: 0.1524 - val_loss: 2.2874 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 181/500\n",
      "101/101 [==============================] - 2s 17ms/step - loss: 2.2981 - acc: 0.1566 - val_loss: 2.2872 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 182/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2984 - acc: 0.1507 - val_loss: 2.2869 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 183/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2985 - acc: 0.1493 - val_loss: 2.2867 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 184/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2969 - acc: 0.1560 - val_loss: 2.2865 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 185/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2968 - acc: 0.1507 - val_loss: 2.2863 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 186/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2982 - acc: 0.1516 - val_loss: 2.2861 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 187/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2974 - acc: 0.1516 - val_loss: 2.2859 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 188/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2959 - acc: 0.1536 - val_loss: 2.2857 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 189/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2968 - acc: 0.1526 - val_loss: 2.2854 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 190/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2966 - acc: 0.1513 - val_loss: 2.2852 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 191/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2956 - acc: 0.1530 - val_loss: 2.2850 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 192/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2960 - acc: 0.1513 - val_loss: 2.2848 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 193/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2953 - acc: 0.1558 - val_loss: 2.2846 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 194/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2968 - acc: 0.1520 - val_loss: 2.2844 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 195/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2958 - acc: 0.1525 - val_loss: 2.2842 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 196/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2960 - acc: 0.1545 - val_loss: 2.2840 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 197/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2967 - acc: 0.1498 - val_loss: 2.2838 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 198/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2957 - acc: 0.1523 - val_loss: 2.2836 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 199/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2964 - acc: 0.1479 - val_loss: 2.2834 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 200/500\n",
      "101/101 [==============================] - 2s 20ms/step - loss: 2.2957 - acc: 0.1520 - val_loss: 2.2832 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 201/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2941 - acc: 0.1534 - val_loss: 2.2830 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 202/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2958 - acc: 0.1538 - val_loss: 2.2828 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 203/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2953 - acc: 0.1509 - val_loss: 2.2825 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 204/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2948 - acc: 0.1564 - val_loss: 2.2823 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 205/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2941 - acc: 0.1518 - val_loss: 2.2821 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 206/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2941 - acc: 0.1505 - val_loss: 2.2819 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 207/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2938 - acc: 0.1502 - val_loss: 2.2817 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 208/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2939 - acc: 0.1526 - val_loss: 2.2815 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 209/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2935 - acc: 0.1550 - val_loss: 2.2813 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 210/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2943 - acc: 0.1517 - val_loss: 2.2811 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 211/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2936 - acc: 0.1493 - val_loss: 2.2809 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 212/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2939 - acc: 0.1494 - val_loss: 2.2807 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 213/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2939 - acc: 0.1537 - val_loss: 2.2805 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 214/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2937 - acc: 0.1548 - val_loss: 2.2803 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 215/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2935 - acc: 0.1471 - val_loss: 2.2801 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 216/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2928 - acc: 0.1489 - val_loss: 2.2799 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 217/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2928 - acc: 0.1540 - val_loss: 2.2797 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 218/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2926 - acc: 0.1485 - val_loss: 2.2795 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 219/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2935 - acc: 0.1484 - val_loss: 2.2794 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 220/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2927 - acc: 0.1510 - val_loss: 2.2792 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 221/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2939 - acc: 0.1511 - val_loss: 2.2790 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 222/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2927 - acc: 0.1507 - val_loss: 2.2788 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 223/500\n",
      "101/101 [==============================] - 2s 20ms/step - loss: 2.2916 - acc: 0.1534 - val_loss: 2.2786 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 224/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2918 - acc: 0.1535 - val_loss: 2.2784 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 225/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2929 - acc: 0.1504 - val_loss: 2.2782 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 226/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2919 - acc: 0.1510 - val_loss: 2.2780 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 227/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2904 - acc: 0.1572 - val_loss: 2.2778 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 228/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2906 - acc: 0.1525 - val_loss: 2.2776 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 229/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2916 - acc: 0.1530 - val_loss: 2.2774 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 230/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2905 - acc: 0.1550 - val_loss: 2.2772 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 231/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2922 - acc: 0.1515 - val_loss: 2.2771 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 232/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2904 - acc: 0.1527 - val_loss: 2.2769 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 233/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2901 - acc: 0.1491 - val_loss: 2.2767 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 234/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2897 - acc: 0.1532 - val_loss: 2.2765 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 235/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2911 - acc: 0.1521 - val_loss: 2.2763 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 236/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2929 - acc: 0.1484 - val_loss: 2.2761 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 237/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2895 - acc: 0.1512 - val_loss: 2.2759 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 238/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2887 - acc: 0.1506 - val_loss: 2.2757 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 239/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2895 - acc: 0.1546 - val_loss: 2.2755 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 240/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2908 - acc: 0.1498 - val_loss: 2.2753 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 241/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2889 - acc: 0.1542 - val_loss: 2.2751 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 242/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2907 - acc: 0.1515 - val_loss: 2.2750 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 243/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2904 - acc: 0.1497 - val_loss: 2.2748 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 244/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2885 - acc: 0.1532 - val_loss: 2.2746 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 245/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2881 - acc: 0.1513 - val_loss: 2.2744 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 246/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2892 - acc: 0.1497 - val_loss: 2.2742 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 247/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2888 - acc: 0.1527 - val_loss: 2.2740 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 248/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2890 - acc: 0.1507 - val_loss: 2.2738 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 249/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2876 - acc: 0.1583 - val_loss: 2.2736 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 250/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2868 - acc: 0.1569 - val_loss: 2.2735 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 251/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2872 - acc: 0.1550 - val_loss: 2.2733 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 252/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2899 - acc: 0.1470 - val_loss: 2.2731 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 253/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2890 - acc: 0.1510 - val_loss: 2.2729 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 254/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2865 - acc: 0.1538 - val_loss: 2.2727 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 255/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2884 - acc: 0.1528 - val_loss: 2.2725 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 256/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2884 - acc: 0.1500 - val_loss: 2.2724 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 257/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2865 - acc: 0.1555 - val_loss: 2.2722 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 258/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2860 - acc: 0.1517 - val_loss: 2.2720 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 259/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2886 - acc: 0.1517 - val_loss: 2.2718 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 260/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2888 - acc: 0.1498 - val_loss: 2.2716 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 261/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2872 - acc: 0.1509 - val_loss: 2.2715 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 262/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2868 - acc: 0.1496 - val_loss: 2.2713 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 263/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2875 - acc: 0.1517 - val_loss: 2.2711 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 264/500\n",
      "101/101 [==============================] - 2s 20ms/step - loss: 2.2888 - acc: 0.1479 - val_loss: 2.2709 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 265/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2852 - acc: 0.1524 - val_loss: 2.2708 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 266/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2870 - acc: 0.1542 - val_loss: 2.2706 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 267/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2887 - acc: 0.1493 - val_loss: 2.2704 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 268/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2870 - acc: 0.1501 - val_loss: 2.2703 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 269/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2848 - acc: 0.1538 - val_loss: 2.2701 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 270/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2866 - acc: 0.1585 - val_loss: 2.2699 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 271/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2858 - acc: 0.1513 - val_loss: 2.2697 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 272/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2842 - acc: 0.1533 - val_loss: 2.2695 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 273/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2864 - acc: 0.1548 - val_loss: 2.2694 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 274/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2845 - acc: 0.1577 - val_loss: 2.2692 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 275/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2872 - acc: 0.1513 - val_loss: 2.2690 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 276/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2855 - acc: 0.1514 - val_loss: 2.2689 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 277/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2855 - acc: 0.1532 - val_loss: 2.2687 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 278/500\n",
      "101/101 [==============================] - 2s 20ms/step - loss: 2.2855 - acc: 0.1503 - val_loss: 2.2685 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 279/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2848 - acc: 0.1511 - val_loss: 2.2684 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 280/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2857 - acc: 0.1495 - val_loss: 2.2682 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 281/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2856 - acc: 0.1515 - val_loss: 2.2680 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 282/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2848 - acc: 0.1503 - val_loss: 2.2679 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 283/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2856 - acc: 0.1504 - val_loss: 2.2677 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 284/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2851 - acc: 0.1473 - val_loss: 2.2676 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 285/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2833 - acc: 0.1533 - val_loss: 2.2674 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 286/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2849 - acc: 0.1528 - val_loss: 2.2672 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 287/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2838 - acc: 0.1530 - val_loss: 2.2670 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 288/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2840 - acc: 0.1494 - val_loss: 2.2669 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 289/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2838 - acc: 0.1525 - val_loss: 2.2667 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 290/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2832 - acc: 0.1553 - val_loss: 2.2665 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 291/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2843 - acc: 0.1528 - val_loss: 2.2664 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 292/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2828 - acc: 0.1494 - val_loss: 2.2662 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 293/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2812 - acc: 0.1552 - val_loss: 2.2660 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 294/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2826 - acc: 0.1555 - val_loss: 2.2659 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 295/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2834 - acc: 0.1486 - val_loss: 2.2657 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 296/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2827 - acc: 0.1508 - val_loss: 2.2655 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 297/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2843 - acc: 0.1499 - val_loss: 2.2654 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 298/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2818 - acc: 0.1558 - val_loss: 2.2652 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 299/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2840 - acc: 0.1484 - val_loss: 2.2651 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 300/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2824 - acc: 0.1535 - val_loss: 2.2649 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 301/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2820 - acc: 0.1523 - val_loss: 2.2647 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 302/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2831 - acc: 0.1468 - val_loss: 2.2646 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 303/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2818 - acc: 0.1536 - val_loss: 2.2644 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 304/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2814 - acc: 0.1495 - val_loss: 2.2642 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 305/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2807 - acc: 0.1534 - val_loss: 2.2641 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 306/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2836 - acc: 0.1506 - val_loss: 2.2639 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 307/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2829 - acc: 0.1499 - val_loss: 2.2638 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 308/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2818 - acc: 0.1521 - val_loss: 2.2636 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 309/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2834 - acc: 0.1484 - val_loss: 2.2635 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 310/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2811 - acc: 0.1532 - val_loss: 2.2633 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 311/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2814 - acc: 0.1527 - val_loss: 2.2632 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 312/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2814 - acc: 0.1526 - val_loss: 2.2630 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 313/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2820 - acc: 0.1497 - val_loss: 2.2629 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 314/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2786 - acc: 0.1509 - val_loss: 2.2627 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 315/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2792 - acc: 0.1572 - val_loss: 2.2625 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 316/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2811 - acc: 0.1502 - val_loss: 2.2624 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 317/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2785 - acc: 0.1506 - val_loss: 2.2622 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 318/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2804 - acc: 0.1541 - val_loss: 2.2620 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 319/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2788 - acc: 0.1540 - val_loss: 2.2619 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 320/500\n",
      "101/101 [==============================] - 2s 18ms/step - loss: 2.2784 - acc: 0.1537 - val_loss: 2.2617 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 321/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2789 - acc: 0.1505 - val_loss: 2.2615 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 322/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2788 - acc: 0.1536 - val_loss: 2.2614 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 323/500\n",
      "101/101 [==============================] - 2s 19ms/step - loss: 2.2801 - acc: 0.1504 - val_loss: 2.2612 - val_acc: 0.1523 - lr: 0.0010\n",
      "Epoch 324/500\n",
      " 58/101 [================>.............] - ETA: 0s - loss: 2.2793 - acc: 0.1591"
     ]
    }
   ],
   "source": [
    "training_history = baseline_model.fit(X_train_seq_pad,\n",
    "                                      y_train,\n",
    "                                      epochs=EPOCHS,\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      callbacks=call_back_list,\n",
    "                                      validation_split=Test_Size, \n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = training_history.history[\"loss\"]\n",
    "val_loss = training_history.history[\"val_loss\"]\n",
    "acc = training_history.history[\"acc\"]\n",
    "val_acc = training_history.history[\"val_acc\"]\n",
    "epochs = range(1,len(loss)+1)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.plot(epochs, loss,\"bo\",label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss,\"b\",label=\"Validation Loss\")\n",
    "\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(\"../Documents/Images/loss_curves.png\",dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.plot(epochs, acc,\"go\",label=\"Training Acc\")\n",
    "plt.plot(epochs, val_acc,\"g\",label=\"Validation Acc\")\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(\"../Documents/Images/accuracy_curves.png\",dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge overfit!\n",
    "\n",
    "Things I need to try:\n",
    "\n",
    "* Collect more data\n",
    "* Use a pretrained word embedding\n",
    "* Regularize\n",
    "* Add callbacks to make training work harder for me\n",
    "* which should I use as a loss function: `sparse_categorical_crossentropy` or `categorical_crossentropy`\n",
    "* Which optimizer should I use: `adam` or `RMSprop(lr=0.1)`\n",
    "\n",
    "**TODO**:\n",
    "Find out what it means when val_loss improves by val_acc does not improve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = baseline_model.evaluate(X_test_seq_pad, y_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T18:13:50.940607Z",
     "iopub.status.busy": "2020-11-30T18:13:50.940360Z",
     "iopub.status.idle": "2020-11-30T18:13:50.943187Z",
     "shell.execute_reply": "2020-11-30T18:13:50.942548Z",
     "shell.execute_reply.started": "2020-11-30T18:13:50.940580Z"
    }
   },
   "source": [
    "# Updated Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T18:13:54.540365Z",
     "iopub.status.busy": "2020-11-30T18:13:54.540109Z",
     "iopub.status.idle": "2020-11-30T18:13:54.542941Z",
     "shell.execute_reply": "2020-11-30T18:13:54.542361Z",
     "shell.execute_reply.started": "2020-11-30T18:13:54.540337Z"
    }
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
